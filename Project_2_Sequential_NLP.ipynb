{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project - 2: Sequential NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkQl04TxbT_w"
      },
      "source": [
        "## Project - 2: Sequential NLP\n",
        "\n",
        "**Data Description**\n",
        "\n",
        "* The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). \n",
        "* Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
        "* For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. \n",
        "* Use the first 20 words from each review to speed up training,using a max vocabulary size of 10,000. \n",
        "* As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "\n",
        "**PROJECT OBJECTIVE**\n",
        "\n",
        "* Build a sequential NLP classifier which can use input text parameters to determine the customer sentiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7kZq7DycN0p"
      },
      "source": [
        "### Get our workspace ready\n",
        "\n",
        "* Import TensorFlow 2.x ✅\n",
        "* Import TensorFlow Hub ✅\n",
        "* Make sure we're using a GPU ✅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoL1mqPLcOdT"
      },
      "source": [
        "# Run this cell if TensorFlow 2.x isn't the default in Colab\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZQsnyNOJcTMP",
        "outputId": "2f94fc5e-2c8b-4e2b-a270-66064cac9bca"
      },
      "source": [
        "# Import necessary tools\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub \n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"TF Hub version:\", hub.__version__)\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"GPU\", \"available (YES!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.5.0\n",
            "TF Hub version: 0.12.0\n",
            "GPU available (YES!)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y_4-qcTwceC1",
        "outputId": "5d03a800-ffb5-4135-a994-d58805260d5a"
      },
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHTaWyEjczl6"
      },
      "source": [
        "# Setting the current working directory\n",
        "import os; os.chdir('drive/MyDrive/AIML Data')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cThO5NbHdRSS"
      },
      "source": [
        "# Import packages\n",
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "from itertools import islice\n",
        "\n",
        "# Keras\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras.datasets import imdb\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "\n",
        "random_state = 42\n",
        "np.random.seed(random_state)\n",
        "tf.random.set_seed(random_state)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APWfap-3Ebb0"
      },
      "source": [
        "### Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cihPgEadU-F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2c1612fd-6062-477f-e8ae-932c7653adb9"
      },
      "source": [
        "dataset = imdb.load_data()\n",
        "\n",
        "dataset\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
              "         ...,\n",
              "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz9b38F4bVcf"
      },
      "source": [
        "##Splitting dataset into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEisOYYzE_6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2b873a63-17c4-436e-c4f8-840d1a3fb0ad"
      },
      "source": [
        "vocab_size = 10000\n",
        "maxlen = 300\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen = maxlen, padding = 'pre')\n",
        "x_test =  pad_sequences(x_test, maxlen = maxlen, padding = 'pre')\n",
        "\n",
        "X = np.concatenate((x_train, x_test), axis = 0)\n",
        "y = np.concatenate((y_train, y_test), axis = 0)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state, shuffle = True)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state = random_state, shuffle = True)\n",
        "\n",
        "print('---'*20, f'\\nNumber of rows in training dataset: {x_train.shape[0]}')\n",
        "print(f'Number of columns in training dataset: {x_train.shape[1]}')\n",
        "print(f'Number of unique words in training dataset: {len(np.unique(np.hstack(x_train)))}')\n",
        "\n",
        "\n",
        "print('---'*20, f'\\nNumber of rows in validation dataset: {x_valid.shape[0]}')\n",
        "print(f'Number of columns in validation dataset: {x_valid.shape[1]}')\n",
        "print(f'Number of unique words in validation dataset: {len(np.unique(np.hstack(x_valid)))}')\n",
        "\n",
        "\n",
        "print('---'*20, f'\\nNumber of rows in test dataset: {x_test.shape[0]}')\n",
        "print(f'Number of columns in test dataset: {x_test.shape[1]}')\n",
        "print(f'Number of unique words in test dataset: {len(np.unique(np.hstack(x_test)))}')\n",
        "\n",
        "\n",
        "print('---'*20, f'\\nUnique Categories: {np.unique(y_train), np.unique(y_valid), np.unique(y_test)}')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------ \n",
            "Number of rows in training dataset: 32000\n",
            "Number of columns in training dataset: 300\n",
            "Number of unique words in training dataset: 9999\n",
            "------------------------------------------------------------ \n",
            "Number of rows in validation dataset: 8000\n",
            "Number of columns in validation dataset: 300\n",
            "Number of unique words in validation dataset: 9984\n",
            "------------------------------------------------------------ \n",
            "Number of rows in test dataset: 10000\n",
            "Number of columns in test dataset: 300\n",
            "Number of unique words in test dataset: 9995\n",
            "------------------------------------------------------------ \n",
            "Unique Categories: (array([0, 1]), array([0, 1]), array([0, 1]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCGlgizJb7F4"
      },
      "source": [
        "### Get word index and create a key-value pair for word and word id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suOyogpBb77N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "69f0baef-e20b-491e-fdc9-99674525c438"
      },
      "source": [
        "def decode_review(x, y):\n",
        "  w2i = imdb.get_word_index()                                \n",
        "  w2i = {k:(v + 3) for k, v in w2i.items()}\n",
        "  w2i['<PAD>'] = 0\n",
        "  w2i['<START>'] = 1\n",
        "  w2i['<UNK>'] = 2\n",
        "  i2w = {i: w for w, i in w2i.items()}\n",
        "\n",
        "  ws = (' '.join(i2w[i] for i in x))\n",
        "  print(f'Review: {ws}')\n",
        "  print(f'Actual Sentiment: {y}')\n",
        "  return w2i, i2w\n",
        "\n",
        "w2i, i2w = decode_review(x_train[0], y_train[0])\n",
        "\n",
        "# get first 50 key, value pairs from id to word dictionary\n",
        "print('---'*30, '\\n', list(islice(i2w.items(), 0, 50)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> the only possible way to enjoy this flick is to bang your head against the wall allow some internal <UNK> of the brain let a bunch of your brain cells die and once you are officially mentally retarded perhaps then you might enjoy this film br br the only saving grace was the story between <UNK> and stephanie govinda was excellent in the role of the cab driver and so was the brit girl perhaps if they would have created the whole movie on their <UNK> in india and how they eventually fall in love would have made it a much more enjoyable film br br the only reason i gave it a 3 rating is because of <UNK> and his ability as an actor when it comes to comedy br br <UNK> <UNK> and anil kapoor were wasted needlessly plus the scene at <UNK> of the re union was just too much to <UNK> being an international <UNK> in the post 9 11 world anil kapoor would have got himself shot much before he even reached the sky bridge to <UNK> his true love but then again the point of the movie was to defy logic gravity physics and throw an egg on the face of the general audience br br watch it at your own peril at least i know i have been <UNK> for life\n",
            "Actual Sentiment: 0\n",
            "------------------------------------------------------------------------------------------ \n",
            " [(34704, 'fawn'), (52009, 'tsukino'), (52010, 'nunnery'), (16819, 'sonja'), (63954, 'vani'), (1411, 'woods'), (16118, 'spiders'), (2348, 'hanging'), (2292, 'woody'), (52011, 'trawling'), (52012, \"hold's\"), (11310, 'comically'), (40833, 'localized'), (30571, 'disobeying'), (52013, \"'royale\"), (40834, \"harpo's\"), (52014, 'canet'), (19316, 'aileen'), (52015, 'acurately'), (52016, \"diplomat's\"), (25245, 'rickman'), (6749, 'arranged'), (52017, 'rumbustious'), (52018, 'familiarness'), (52019, \"spider'\"), (68807, 'hahahah'), (52020, \"wood'\"), (40836, 'transvestism'), (34705, \"hangin'\"), (2341, 'bringing'), (40837, 'seamier'), (34706, 'wooded'), (52021, 'bravora'), (16820, 'grueling'), (1639, 'wooden'), (16821, 'wednesday'), (52022, \"'prix\"), (34707, 'altagracia'), (52023, 'circuitry'), (11588, 'crotch'), (57769, 'busybody'), (52024, \"tart'n'tangy\"), (14132, 'burgade'), (52026, 'thrace'), (11041, \"tom's\"), (52028, 'snuggles'), (29117, 'francesco'), (52030, 'complainers'), (52128, 'templarios'), (40838, '272')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaa8UYVwOUmN"
      },
      "source": [
        "### Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RsOBlEdcAp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eb8326e6-2d1f-4c95-aa18-9c72b364f426"
      },
      "source": [
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256, input_length = maxlen))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(256, 5, padding = 'same', activation = 'relu', strides = 1))\n",
        "model.add(Conv1D(128, 5, padding = 'same', activation = 'relu', strides = 1))\n",
        "model.add(MaxPooling1D(pool_size = 2))\n",
        "model.add(Conv1D(64, 5, padding = 'same', activation = 'relu', strides = 1))\n",
        "model.add(MaxPooling1D(pool_size = 2))\n",
        "model.add(LSTM(75))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Adding callbacks\n",
        "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 0)  \n",
        "mc = ModelCheckpoint('imdb_model.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300, 256)          0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 300, 256)          327936    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 300, 128)          163968    \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 150, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 150, 64)           41024     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 75, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 75)                42000     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 76        \n",
            "=================================================================\n",
            "Total params: 3,135,004\n",
            "Trainable params: 3,135,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYvJGoCoOcLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d2f63349-5dba-4df1-c058-8e386b2879f2"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(x_train, y_train, validation_data = (x_valid, y_valid), epochs = 4, batch_size = 64, verbose = True, callbacks = [es, mc])\n",
        "\n",
        "# Evaluate the model\n",
        "scores = model.evaluate(x_test, y_test, batch_size = 64)\n",
        "print('Test accuracy: %.2f%%' % (scores[1]*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "500/500 [==============================] - 52s 62ms/step - loss: 0.4645 - accuracy: 0.7413 - val_loss: 0.2553 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.25529, saving model to imdb_model.h5\n",
            "157/157 [==============================] - 3s 15ms/step - loss: 0.2460 - accuracy: 0.9039\n",
            "Test accuracy: 90.39%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STXlNJ9wmOll"
      },
      "source": [
        "## Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr56eHq9biqX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b366071c-3f1b-4587-f754-36aef45455fe"
      },
      "source": [
        "y_pred = model.predict_classes(x_test)\n",
        "print(f'Classification Report:\\n{classification_report(y_pred, y_test)}')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.94      0.90      4539\n",
            "           1       0.94      0.88      0.91      5461\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.91      0.90     10000\n",
            "weighted avg       0.91      0.90      0.90     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLn896G8n8ws"
      },
      "source": [
        "## Retrive output of each layer in keras for a given single test sample from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ibtwf73n9pw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "83fd3fe8-a3a6-4b5f-eb22-c948250b400d"
      },
      "source": [
        "sample_x_test = x_test[np.random.randint(10000)]\n",
        "for layer in model.layers:\n",
        "\n",
        "    model_layer = Model(inputs = model.input, outputs = model.get_layer(layer.name).output)\n",
        "    output = model_layer.predict(sample_x_test.reshape(1,-1))\n",
        "    print('\\n','--'*20, layer.name, 'layer', '--'*20, '\\n')\n",
        "    print(output)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ---------------------------------------- embedding layer ---------------------------------------- \n",
            "\n",
            "[[[ 0.0153265   0.01457338 -0.00354812 ... -0.00121397  0.03002104\n",
            "    0.01639611]\n",
            "  [ 0.0153265   0.01457338 -0.00354812 ... -0.00121397  0.03002104\n",
            "    0.01639611]\n",
            "  [ 0.0153265   0.01457338 -0.00354812 ... -0.00121397  0.03002104\n",
            "    0.01639611]\n",
            "  ...\n",
            "  [-0.02284429 -0.00694898 -0.02989902 ... -0.04918609  0.00354121\n",
            "   -0.05747491]\n",
            "  [-0.01846446  0.01192249 -0.00708421 ...  0.03047889 -0.00650249\n",
            "    0.02461625]\n",
            "  [ 0.02135502 -0.06917109  0.01016708 ... -0.01707494 -0.03806091\n",
            "    0.01850636]]]\n",
            "\n",
            " ---------------------------------------- dropout layer ---------------------------------------- \n",
            "\n",
            "[[[ 0.0153265   0.01457338 -0.00354812 ... -0.00121397  0.03002104\n",
            "    0.01639611]\n",
            "  [ 0.0153265   0.01457338 -0.00354812 ... -0.00121397  0.03002104\n",
            "    0.01639611]\n",
            "  [ 0.0153265   0.01457338 -0.00354812 ... -0.00121397  0.03002104\n",
            "    0.01639611]\n",
            "  ...\n",
            "  [-0.02284429 -0.00694898 -0.02989902 ... -0.04918609  0.00354121\n",
            "   -0.05747491]\n",
            "  [-0.01846446  0.01192249 -0.00708421 ...  0.03047889 -0.00650249\n",
            "    0.02461625]\n",
            "  [ 0.02135502 -0.06917109  0.01016708 ... -0.01707494 -0.03806091\n",
            "    0.01850636]]]\n",
            "\n",
            " ---------------------------------------- conv1d layer ---------------------------------------- \n",
            "\n",
            "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.         0.01432737 0.04087543 ... 0.         0.00961708 0.03184832]\n",
            "  [0.0168167  0.         0.03817679 ... 0.01161848 0.         0.0116732 ]\n",
            "  [0.         0.02315924 0.         ... 0.         0.         0.        ]]]\n",
            "WARNING:tensorflow:5 out of the last 317 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f300a653b90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\n",
            " ---------------------------------------- conv1d_1 layer ---------------------------------------- \n",
            "\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "WARNING:tensorflow:6 out of the last 318 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f300a4aa200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\n",
            " ---------------------------------------- max_pooling1d layer ---------------------------------------- \n",
            "\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            " ---------------------------------------- conv1d_2 layer ---------------------------------------- \n",
            "\n",
            "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
            "  ...\n",
            "  [0.         0.69340616 0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.3147948  0.         ... 0.         0.         0.        ]\n",
            "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
            "\n",
            " ---------------------------------------- max_pooling1d_1 layer ---------------------------------------- \n",
            "\n",
            "[[[0.        0.        0.        ... 0.        0.        0.       ]\n",
            "  [0.        0.        0.        ... 0.        0.        0.       ]\n",
            "  [0.        0.        0.        ... 0.        0.        0.       ]\n",
            "  ...\n",
            "  [0.        2.334105  0.        ... 0.        0.        0.       ]\n",
            "  [0.        1.4189682 0.        ... 0.        0.        0.       ]\n",
            "  [0.        0.3147948 0.        ... 0.        0.        0.       ]]]\n",
            "\n",
            " ---------------------------------------- lstm layer ---------------------------------------- \n",
            "\n",
            "[[ 0.5355204   0.45888653 -0.38325933  0.33078444 -0.5573702  -0.3792595\n",
            "  -0.46279302  0.05537514  0.03949961  0.09651005 -0.3325942   0.33073267\n",
            "  -0.05906634 -0.11528623  0.0062535   0.10266829  0.38571587 -0.36642846\n",
            "   0.15566756 -0.4991434  -0.08190857 -0.45028967 -0.20005848 -0.57486564\n",
            "   0.49258566  0.5568513   0.20062277 -0.09537186 -0.47506753  0.03766495\n",
            "  -0.09855288 -0.5047693   0.40785712 -0.00243572 -0.36997172  0.00420348\n",
            "   0.40197772 -0.11799118 -0.15400015  0.41579783  0.28461587 -0.4648998\n",
            "   0.38778812  0.0637444  -0.4987643  -0.4628508  -0.00965665  0.46480098\n",
            "  -0.11498812  0.45882857 -0.06767596  0.3901537  -0.43505847 -0.37905368\n",
            "   0.34337693  0.0589413   0.31894305 -0.00634416 -0.53153014 -0.45966783\n",
            "   0.31712186 -0.25066957 -0.02799229  0.01500191 -0.31060013  0.535413\n",
            "   0.40580174 -0.3609468   0.5018229   0.36222708 -0.42958197  0.36963415\n",
            "  -0.54142016 -0.08372986 -0.03157884]]\n",
            "\n",
            " ---------------------------------------- dense layer ---------------------------------------- \n",
            "\n",
            "[[0.03778912]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjJkFqe-oGi0"
      },
      "source": [
        "## Predict Seniments from the Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_IRR3U8oBeg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "711e4e1a-ca29-41b8-dff0-429e98aeb3ec"
      },
      "source": [
        "decode_review(x_test[10], y_test[10])\n",
        "print(f'Predicted sentiment: {y_pred[10][0]}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this movie was great and i was waiting for it for a long time when it finally came out i was really happy and looked forward to a 10 out of 10 it was great and lived up to my potential the performances were great on the part of the adults and most of the kids the only bad performance was by milo himself there was one problem that i encountered with this and others like it movie all of the characters i wanted to live were getting killed overall i give this movie an excellent 9 out of 10 maybe we should <UNK> better people to kill next time though ok\n",
            "Actual Sentiment: 1\n",
            "Predicted sentiment: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CafjzopDtR2j"
      },
      "source": [
        "### Conclusion\n",
        "* Sentiment classification task on the IMDB dataset, on test dataset,\n",
        "  * Accuracy: > 90%\n",
        "  * F1-score: > 90%\n",
        "  * Loss of 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TlY5gwcoNz5"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}